// THE AUTOGENERATED LICENSE. ALL THE RIGHTS ARE RESERVED BY ROBOTS.

// WARNING: This file has automatically been generated on Tue, 13 Dec 2016 00:30:19 GMT.
// By https://git.io/cgogen. DO NOT EDIT.

package gen

/*
#include <linux/perf_event.h>
#include <stdlib.h>
#include "cgo_helpers.h"
*/
import "C"
import (
	"sync"
	"unsafe"
)

// cgoAllocMap stores pointers to C allocated memory for future reference.
type cgoAllocMap struct {
	mux sync.RWMutex
	m   map[unsafe.Pointer]struct{}
}

var cgoAllocsUnknown = new(cgoAllocMap)

func (a *cgoAllocMap) Add(ptr unsafe.Pointer) {
	a.mux.Lock()
	if a.m == nil {
		a.m = make(map[unsafe.Pointer]struct{})
	}
	a.m[ptr] = struct{}{}
	a.mux.Unlock()
}

func (a *cgoAllocMap) IsEmpty() bool {
	a.mux.RLock()
	isEmpty := len(a.m) == 0
	a.mux.RUnlock()
	return isEmpty
}

func (a *cgoAllocMap) Borrow(b *cgoAllocMap) {
	if b == nil || b.IsEmpty() {
		return
	}
	b.mux.Lock()
	a.mux.Lock()
	for ptr := range b.m {
		if a.m == nil {
			a.m = make(map[unsafe.Pointer]struct{})
		}
		a.m[ptr] = struct{}{}
		delete(b.m, ptr)
	}
	a.mux.Unlock()
	b.mux.Unlock()
}

func (a *cgoAllocMap) Free() {
	a.mux.Lock()
	for ptr := range a.m {
		C.free(ptr)
		delete(a.m, ptr)
	}
	a.mux.Unlock()
}

// allocStructPerfEventMmapPageMemory allocates memory for type C.struct_perf_event_mmap_page in C.
// The caller is responsible for freeing the this memory via C.free.
func allocStructPerfEventMmapPageMemory(n int) unsafe.Pointer {
	mem, err := C.calloc(C.size_t(n), (C.size_t)(sizeOfStructPerfEventMmapPageValue))
	if err != nil {
		panic("memory alloc error: " + err.Error())
	}
	return mem
}

const sizeOfStructPerfEventMmapPageValue = unsafe.Sizeof([1]C.struct_perf_event_mmap_page{})

// Ref returns the underlying reference to C object or nil if struct is nil.
func (x *MmapPage) Ref() *C.struct_perf_event_mmap_page {
	if x == nil {
		return nil
	}
	return x.refc3d57b4c
}

// Free invokes alloc map's free mechanism that cleanups any allocated memory using C free.
// Does nothing if struct is nil or has no allocation map.
func (x *MmapPage) Free() {
	if x != nil && x.allocsc3d57b4c != nil {
		x.allocsc3d57b4c.(*cgoAllocMap).Free()
		x.refc3d57b4c = nil
	}
}

// NewMmapPageRef creates a new wrapper struct with underlying reference set to the original C object.
// Returns nil if the provided pointer to C object is nil too.
func NewMmapPageRef(ref unsafe.Pointer) *MmapPage {
	if ref == nil {
		return nil
	}
	obj := new(MmapPage)
	obj.refc3d57b4c = (*C.struct_perf_event_mmap_page)(unsafe.Pointer(ref))
	return obj
}

// PassRef returns the underlying C object, otherwise it will allocate one and set its values
// from this wrapping struct, counting allocations into an allocation map.
func (x *MmapPage) PassRef() (*C.struct_perf_event_mmap_page, *cgoAllocMap) {
	if x == nil {
		return nil, nil
	} else if x.refc3d57b4c != nil {
		return x.refc3d57b4c, nil
	}
	memc3d57b4c := allocStructPerfEventMmapPageMemory(1)
	refc3d57b4c := (*C.struct_perf_event_mmap_page)(memc3d57b4c)
	allocsc3d57b4c := new(cgoAllocMap)
	var cversion_allocs *cgoAllocMap
	refc3d57b4c.version, cversion_allocs = (C.__u32)(x.Version), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cversion_allocs)

	var ccompat_version_allocs *cgoAllocMap
	refc3d57b4c.compat_version, ccompat_version_allocs = (C.__u32)(x.CompatVersion), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ccompat_version_allocs)

	var clock_allocs *cgoAllocMap
	refc3d57b4c.lock, clock_allocs = (C.__u32)(x.Lock), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(clock_allocs)

	var cindex_allocs *cgoAllocMap
	refc3d57b4c.index, cindex_allocs = (C.__u32)(x.Index), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cindex_allocs)

	var coffset_allocs *cgoAllocMap
	refc3d57b4c.offset, coffset_allocs = (C.__s64)(x.Offset), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(coffset_allocs)

	var ctime_enabled_allocs *cgoAllocMap
	refc3d57b4c.time_enabled, ctime_enabled_allocs = (C.__u64)(x.TimeEnabled), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_enabled_allocs)

	var ctime_running_allocs *cgoAllocMap
	refc3d57b4c.time_running, ctime_running_allocs = (C.__u64)(x.TimeRunning), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_running_allocs)

	var cpmc_width_allocs *cgoAllocMap
	refc3d57b4c.pmc_width, cpmc_width_allocs = (C.__u16)(x.PMCWidth), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cpmc_width_allocs)

	var ctime_shift_allocs *cgoAllocMap
	refc3d57b4c.time_shift, ctime_shift_allocs = (C.__u16)(x.TimeShift), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_shift_allocs)

	var ctime_mult_allocs *cgoAllocMap
	refc3d57b4c.time_mult, ctime_mult_allocs = (C.__u32)(x.TimeMult), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_mult_allocs)

	var ctime_offset_allocs *cgoAllocMap
	refc3d57b4c.time_offset, ctime_offset_allocs = (C.__u64)(x.TimeOffset), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_offset_allocs)

	var ctime_zero_allocs *cgoAllocMap
	refc3d57b4c.time_zero, ctime_zero_allocs = (C.__u64)(x.TimeZero), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(ctime_zero_allocs)

	var csize_allocs *cgoAllocMap
	refc3d57b4c.size, csize_allocs = (C.__u32)(x.Size), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(csize_allocs)

	var c__reserved_allocs *cgoAllocMap
	refc3d57b4c.__reserved, c__reserved_allocs = *(*[948]C.__u8)(unsafe.Pointer(&x._Reserved)), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(c__reserved_allocs)

	var cdata_head_allocs *cgoAllocMap
	refc3d57b4c.data_head, cdata_head_allocs = (C.__u64)(x.DataHead), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cdata_head_allocs)

	var cdata_tail_allocs *cgoAllocMap
	refc3d57b4c.data_tail, cdata_tail_allocs = (C.__u64)(x.DataTail), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cdata_tail_allocs)

	var cdata_offset_allocs *cgoAllocMap
	refc3d57b4c.data_offset, cdata_offset_allocs = (C.__u64)(x.DataOffset), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cdata_offset_allocs)

	var cdata_size_allocs *cgoAllocMap
	refc3d57b4c.data_size, cdata_size_allocs = (C.__u64)(x.DataSize), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(cdata_size_allocs)

	var caux_head_allocs *cgoAllocMap
	refc3d57b4c.aux_head, caux_head_allocs = (C.__u64)(x.AuxHead), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(caux_head_allocs)

	var caux_tail_allocs *cgoAllocMap
	refc3d57b4c.aux_tail, caux_tail_allocs = (C.__u64)(x.AuxTail), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(caux_tail_allocs)

	var caux_offset_allocs *cgoAllocMap
	refc3d57b4c.aux_offset, caux_offset_allocs = (C.__u64)(x.AuxOffset), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(caux_offset_allocs)

	var caux_size_allocs *cgoAllocMap
	refc3d57b4c.aux_size, caux_size_allocs = (C.__u64)(x.AuxSize), cgoAllocsUnknown
	allocsc3d57b4c.Borrow(caux_size_allocs)

	x.refc3d57b4c = refc3d57b4c
	x.allocsc3d57b4c = allocsc3d57b4c
	return refc3d57b4c, allocsc3d57b4c

}

// PassValue does the same as PassRef except that it will try to dereference the returned pointer.
func (x MmapPage) PassValue() (C.struct_perf_event_mmap_page, *cgoAllocMap) {
	if x.refc3d57b4c != nil {
		return *x.refc3d57b4c, nil
	}
	ref, allocs := x.PassRef()
	return *ref, allocs
}

// Deref uses the underlying reference to C object and fills the wrapping struct with values.
// Do not forget to call this method whether you get a struct for C object and want to read its values.
func (x *MmapPage) Deref() {
	if x.refc3d57b4c == nil {
		return
	}
	x.Version = (uint32)(x.refc3d57b4c.version)
	x.CompatVersion = (uint32)(x.refc3d57b4c.compat_version)
	x.Lock = (uint32)(x.refc3d57b4c.lock)
	x.Index = (uint32)(x.refc3d57b4c.index)
	x.Offset = (int64)(x.refc3d57b4c.offset)
	x.TimeEnabled = (uint64)(x.refc3d57b4c.time_enabled)
	x.TimeRunning = (uint64)(x.refc3d57b4c.time_running)
	x.PMCWidth = (uint16)(x.refc3d57b4c.pmc_width)
	x.TimeShift = (uint16)(x.refc3d57b4c.time_shift)
	x.TimeMult = (uint32)(x.refc3d57b4c.time_mult)
	x.TimeOffset = (uint64)(x.refc3d57b4c.time_offset)
	x.TimeZero = (uint64)(x.refc3d57b4c.time_zero)
	x.Size = (uint32)(x.refc3d57b4c.size)
	x._Reserved = *(*[948]byte)(unsafe.Pointer(&x.refc3d57b4c.__reserved))
	x.DataHead = (uint64)(x.refc3d57b4c.data_head)
	x.DataTail = (uint64)(x.refc3d57b4c.data_tail)
	x.DataOffset = (uint64)(x.refc3d57b4c.data_offset)
	x.DataSize = (uint64)(x.refc3d57b4c.data_size)
	x.AuxHead = (uint64)(x.refc3d57b4c.aux_head)
	x.AuxTail = (uint64)(x.refc3d57b4c.aux_tail)
	x.AuxOffset = (uint64)(x.refc3d57b4c.aux_offset)
	x.AuxSize = (uint64)(x.refc3d57b4c.aux_size)
}

// allocStructPerfEventHeaderMemory allocates memory for type C.struct_perf_event_header in C.
// The caller is responsible for freeing the this memory via C.free.
func allocStructPerfEventHeaderMemory(n int) unsafe.Pointer {
	mem, err := C.calloc(C.size_t(n), (C.size_t)(sizeOfStructPerfEventHeaderValue))
	if err != nil {
		panic("memory alloc error: " + err.Error())
	}
	return mem
}

const sizeOfStructPerfEventHeaderValue = unsafe.Sizeof([1]C.struct_perf_event_header{})

// Ref returns the underlying reference to C object or nil if struct is nil.
func (x *Header) Ref() *C.struct_perf_event_header {
	if x == nil {
		return nil
	}
	return x.ref82c78663
}

// Free invokes alloc map's free mechanism that cleanups any allocated memory using C free.
// Does nothing if struct is nil or has no allocation map.
func (x *Header) Free() {
	if x != nil && x.allocs82c78663 != nil {
		x.allocs82c78663.(*cgoAllocMap).Free()
		x.ref82c78663 = nil
	}
}

// NewHeaderRef creates a new wrapper struct with underlying reference set to the original C object.
// Returns nil if the provided pointer to C object is nil too.
func NewHeaderRef(ref unsafe.Pointer) *Header {
	if ref == nil {
		return nil
	}
	obj := new(Header)
	obj.ref82c78663 = (*C.struct_perf_event_header)(unsafe.Pointer(ref))
	return obj
}

// PassRef returns the underlying C object, otherwise it will allocate one and set its values
// from this wrapping struct, counting allocations into an allocation map.
func (x *Header) PassRef() (*C.struct_perf_event_header, *cgoAllocMap) {
	if x == nil {
		return nil, nil
	} else if x.ref82c78663 != nil {
		return x.ref82c78663, nil
	}
	mem82c78663 := allocStructPerfEventHeaderMemory(1)
	ref82c78663 := (*C.struct_perf_event_header)(mem82c78663)
	allocs82c78663 := new(cgoAllocMap)
	var c_type_allocs *cgoAllocMap
	ref82c78663._type, c_type_allocs = (C.__u32)(x.Type), cgoAllocsUnknown
	allocs82c78663.Borrow(c_type_allocs)

	var cmisc_allocs *cgoAllocMap
	ref82c78663.misc, cmisc_allocs = (C.__u16)(x.Misc), cgoAllocsUnknown
	allocs82c78663.Borrow(cmisc_allocs)

	var csize_allocs *cgoAllocMap
	ref82c78663.size, csize_allocs = (C.__u16)(x.Size), cgoAllocsUnknown
	allocs82c78663.Borrow(csize_allocs)

	x.ref82c78663 = ref82c78663
	x.allocs82c78663 = allocs82c78663
	return ref82c78663, allocs82c78663

}

// PassValue does the same as PassRef except that it will try to dereference the returned pointer.
func (x Header) PassValue() (C.struct_perf_event_header, *cgoAllocMap) {
	if x.ref82c78663 != nil {
		return *x.ref82c78663, nil
	}
	ref, allocs := x.PassRef()
	return *ref, allocs
}

// Deref uses the underlying reference to C object and fills the wrapping struct with values.
// Do not forget to call this method whether you get a struct for C object and want to read its values.
func (x *Header) Deref() {
	if x.ref82c78663 == nil {
		return
	}
	x.Type = (uint32)(x.ref82c78663._type)
	x.Misc = (uint16)(x.ref82c78663.misc)
	x.Size = (uint16)(x.ref82c78663.size)
}

// allocStructPerfEventAttrMemory allocates memory for type C.struct_perf_event_attr in C.
// The caller is responsible for freeing the this memory via C.free.
func allocStructPerfEventAttrMemory(n int) unsafe.Pointer {
	mem, err := C.calloc(C.size_t(n), (C.size_t)(sizeOfStructPerfEventAttrValue))
	if err != nil {
		panic("memory alloc error: " + err.Error())
	}
	return mem
}

const sizeOfStructPerfEventAttrValue = unsafe.Sizeof([1]C.struct_perf_event_attr{})

// Ref returns the underlying reference to C object or nil if struct is nil.
func (x *Attr) Ref() *C.struct_perf_event_attr {
	if x == nil {
		return nil
	}
	return x.refe590ab4
}

// Free invokes alloc map's free mechanism that cleanups any allocated memory using C free.
// Does nothing if struct is nil or has no allocation map.
func (x *Attr) Free() {
	if x != nil && x.allocse590ab4 != nil {
		x.allocse590ab4.(*cgoAllocMap).Free()
		x.refe590ab4 = nil
	}
}

// NewAttrRef creates a new wrapper struct with underlying reference set to the original C object.
// Returns nil if the provided pointer to C object is nil too.
func NewAttrRef(ref unsafe.Pointer) *Attr {
	if ref == nil {
		return nil
	}
	obj := new(Attr)
	obj.refe590ab4 = (*C.struct_perf_event_attr)(unsafe.Pointer(ref))
	return obj
}

// PassRef returns the underlying C object, otherwise it will allocate one and set its values
// from this wrapping struct, counting allocations into an allocation map.
func (x *Attr) PassRef() (*C.struct_perf_event_attr, *cgoAllocMap) {
	if x == nil {
		return nil, nil
	} else if x.refe590ab4 != nil {
		return x.refe590ab4, nil
	}
	meme590ab4 := allocStructPerfEventAttrMemory(1)
	refe590ab4 := (*C.struct_perf_event_attr)(meme590ab4)
	allocse590ab4 := new(cgoAllocMap)
	var c_type_allocs *cgoAllocMap
	refe590ab4._type, c_type_allocs = (C.__u32)(x.Type), cgoAllocsUnknown
	allocse590ab4.Borrow(c_type_allocs)

	var csize_allocs *cgoAllocMap
	refe590ab4.size, csize_allocs = (C.__u32)(x.Size), cgoAllocsUnknown
	allocse590ab4.Borrow(csize_allocs)

	var cconfig_allocs *cgoAllocMap
	refe590ab4.config, cconfig_allocs = (C.__u64)(x.Config), cgoAllocsUnknown
	allocse590ab4.Borrow(cconfig_allocs)

	var csample_type_allocs *cgoAllocMap
	refe590ab4.sample_type, csample_type_allocs = (C.__u64)(x.SampleType), cgoAllocsUnknown
	allocse590ab4.Borrow(csample_type_allocs)

	var cread_format_allocs *cgoAllocMap
	refe590ab4.read_format, cread_format_allocs = (C.__u64)(x.ReadFormat), cgoAllocsUnknown
	allocse590ab4.Borrow(cread_format_allocs)

	C.set_bitfield_perf_event_attr_disabled(refe590ab4, (C.int)(x.Disabled))

	C.set_bitfield_perf_event_attr_inherit(refe590ab4, (C.int)(x.Inherit))

	C.set_bitfield_perf_event_attr_pinned(refe590ab4, (C.int)(x.Pinned))

	C.set_bitfield_perf_event_attr_exclusive(refe590ab4, (C.int)(x.Exclusive))

	C.set_bitfield_perf_event_attr_exclude_user(refe590ab4, (C.int)(x.ExcludeUser))

	C.set_bitfield_perf_event_attr_exclude_kernel(refe590ab4, (C.int)(x.ExcludeKernel))

	C.set_bitfield_perf_event_attr_exclude_hv(refe590ab4, (C.int)(x.ExcludeHV))

	C.set_bitfield_perf_event_attr_exclude_idle(refe590ab4, (C.int)(x.ExcludeIdle))

	C.set_bitfield_perf_event_attr_mmap(refe590ab4, (C.int)(x.Mmap))

	C.set_bitfield_perf_event_attr_comm(refe590ab4, (C.int)(x.Comm))

	C.set_bitfield_perf_event_attr_freq(refe590ab4, (C.int)(x.Freq))

	C.set_bitfield_perf_event_attr_inherit_stat(refe590ab4, (C.int)(x.InheritStat))

	C.set_bitfield_perf_event_attr_enable_on_exec(refe590ab4, (C.int)(x.EnableOnExec))

	C.set_bitfield_perf_event_attr_task(refe590ab4, (C.int)(x.Task))

	C.set_bitfield_perf_event_attr_watermark(refe590ab4, (C.int)(x.Watermark))

	C.set_bitfield_perf_event_attr_precise_ip(refe590ab4, (C.int)(x.PreciseIP))

	C.set_bitfield_perf_event_attr_mmap_data(refe590ab4, (C.int)(x.MmapData))

	C.set_bitfield_perf_event_attr_sample_id_all(refe590ab4, (C.int)(x.SampleIDAll))

	C.set_bitfield_perf_event_attr_exclude_host(refe590ab4, (C.int)(x.ExcludeHost))

	C.set_bitfield_perf_event_attr_exclude_guest(refe590ab4, (C.int)(x.ExcludeGuest))

	C.set_bitfield_perf_event_attr_exclude_callchain_kernel(refe590ab4, (C.int)(x.ExcludeCallchainKernel))

	C.set_bitfield_perf_event_attr_exclude_callchain_user(refe590ab4, (C.int)(x.ExcludeCallchainUser))

	C.set_bitfield_perf_event_attr_mmap2(refe590ab4, (C.int)(x.Mmap2))

	C.set_bitfield_perf_event_attr_comm_exec(refe590ab4, (C.int)(x.CommExec))

	C.set_bitfield_perf_event_attr_use_clockid(refe590ab4, (C.int)(x.UseClockID))

	C.set_bitfield_perf_event_attr_context_switch(refe590ab4, (C.int)(x.ContextSwitch))

	C.set_bitfield_perf_event_attr_write_backward(refe590ab4, (C.int)(x.WriteBackward))

	C.set_bitfield_perf_event_attr___reserved_1(refe590ab4, (C.int)(x._Reserved1))

	var cbp_type_allocs *cgoAllocMap
	refe590ab4.bp_type, cbp_type_allocs = (C.__u32)(x.BpType), cgoAllocsUnknown
	allocse590ab4.Borrow(cbp_type_allocs)

	var cbranch_sample_type_allocs *cgoAllocMap
	refe590ab4.branch_sample_type, cbranch_sample_type_allocs = (C.__u64)(x.BranchSampleType), cgoAllocsUnknown
	allocse590ab4.Borrow(cbranch_sample_type_allocs)

	var csample_regs_user_allocs *cgoAllocMap
	refe590ab4.sample_regs_user, csample_regs_user_allocs = (C.__u64)(x.SampleRegsUser), cgoAllocsUnknown
	allocse590ab4.Borrow(csample_regs_user_allocs)

	var csample_stack_user_allocs *cgoAllocMap
	refe590ab4.sample_stack_user, csample_stack_user_allocs = (C.__u32)(x.SampleStackUser), cgoAllocsUnknown
	allocse590ab4.Borrow(csample_stack_user_allocs)

	var cclockid_allocs *cgoAllocMap
	refe590ab4.clockid, cclockid_allocs = (C.__s32)(x.ClockID), cgoAllocsUnknown
	allocse590ab4.Borrow(cclockid_allocs)

	var csample_regs_intr_allocs *cgoAllocMap
	refe590ab4.sample_regs_intr, csample_regs_intr_allocs = (C.__u64)(x.SampleRegsIntr), cgoAllocsUnknown
	allocse590ab4.Borrow(csample_regs_intr_allocs)

	var caux_watermark_allocs *cgoAllocMap
	refe590ab4.aux_watermark, caux_watermark_allocs = (C.__u32)(x.AuxWatermark), cgoAllocsUnknown
	allocse590ab4.Borrow(caux_watermark_allocs)

	var csample_max_stack_allocs *cgoAllocMap
	refe590ab4.sample_max_stack, csample_max_stack_allocs = (C.__u16)(x.SampleMaxStack), cgoAllocsUnknown
	allocse590ab4.Borrow(csample_max_stack_allocs)

	var c__reserved_2_allocs *cgoAllocMap
	refe590ab4.__reserved_2, c__reserved_2_allocs = (C.__u16)(x._Reserved2), cgoAllocsUnknown
	allocse590ab4.Borrow(c__reserved_2_allocs)

	x.refe590ab4 = refe590ab4
	x.allocse590ab4 = allocse590ab4
	return refe590ab4, allocse590ab4

}

// PassValue does the same as PassRef except that it will try to dereference the returned pointer.
func (x Attr) PassValue() (C.struct_perf_event_attr, *cgoAllocMap) {
	if x.refe590ab4 != nil {
		return *x.refe590ab4, nil
	}
	ref, allocs := x.PassRef()
	return *ref, allocs
}

// Deref uses the underlying reference to C object and fills the wrapping struct with values.
// Do not forget to call this method whether you get a struct for C object and want to read its values.
func (x *Attr) Deref() {
	if x.refe590ab4 == nil {
		return
	}
	x.Type = (uint32)(x.refe590ab4._type)
	x.Size = (uint32)(x.refe590ab4.size)
	x.Config = (uint64)(x.refe590ab4.config)
	x.SampleType = (uint64)(x.refe590ab4.sample_type)
	x.ReadFormat = (uint64)(x.refe590ab4.read_format)
	x.Disabled = (uint64)(C.get_bitfield_perf_event_attr_disabled(x.refe590ab4))
	x.Inherit = (uint64)(C.get_bitfield_perf_event_attr_inherit(x.refe590ab4))
	x.Pinned = (uint64)(C.get_bitfield_perf_event_attr_pinned(x.refe590ab4))
	x.Exclusive = (uint64)(C.get_bitfield_perf_event_attr_exclusive(x.refe590ab4))
	x.ExcludeUser = (uint64)(C.get_bitfield_perf_event_attr_exclude_user(x.refe590ab4))
	x.ExcludeKernel = (uint64)(C.get_bitfield_perf_event_attr_exclude_kernel(x.refe590ab4))
	x.ExcludeHV = (uint64)(C.get_bitfield_perf_event_attr_exclude_hv(x.refe590ab4))
	x.ExcludeIdle = (uint64)(C.get_bitfield_perf_event_attr_exclude_idle(x.refe590ab4))
	x.Mmap = (uint64)(C.get_bitfield_perf_event_attr_mmap(x.refe590ab4))
	x.Comm = (uint64)(C.get_bitfield_perf_event_attr_comm(x.refe590ab4))
	x.Freq = (uint64)(C.get_bitfield_perf_event_attr_freq(x.refe590ab4))
	x.InheritStat = (uint64)(C.get_bitfield_perf_event_attr_inherit_stat(x.refe590ab4))
	x.EnableOnExec = (uint64)(C.get_bitfield_perf_event_attr_enable_on_exec(x.refe590ab4))
	x.Task = (uint64)(C.get_bitfield_perf_event_attr_task(x.refe590ab4))
	x.Watermark = (uint64)(C.get_bitfield_perf_event_attr_watermark(x.refe590ab4))
	x.PreciseIP = (uint64)(C.get_bitfield_perf_event_attr_precise_ip(x.refe590ab4))
	x.MmapData = (uint64)(C.get_bitfield_perf_event_attr_mmap_data(x.refe590ab4))
	x.SampleIDAll = (uint64)(C.get_bitfield_perf_event_attr_sample_id_all(x.refe590ab4))
	x.ExcludeHost = (uint64)(C.get_bitfield_perf_event_attr_exclude_host(x.refe590ab4))
	x.ExcludeGuest = (uint64)(C.get_bitfield_perf_event_attr_exclude_guest(x.refe590ab4))
	x.ExcludeCallchainKernel = (uint64)(C.get_bitfield_perf_event_attr_exclude_callchain_kernel(x.refe590ab4))
	x.ExcludeCallchainUser = (uint64)(C.get_bitfield_perf_event_attr_exclude_callchain_user(x.refe590ab4))
	x.Mmap2 = (uint64)(C.get_bitfield_perf_event_attr_mmap2(x.refe590ab4))
	x.CommExec = (uint64)(C.get_bitfield_perf_event_attr_comm_exec(x.refe590ab4))
	x.UseClockID = (uint64)(C.get_bitfield_perf_event_attr_use_clockid(x.refe590ab4))
	x.ContextSwitch = (uint64)(C.get_bitfield_perf_event_attr_context_switch(x.refe590ab4))
	x.WriteBackward = (uint64)(C.get_bitfield_perf_event_attr_write_backward(x.refe590ab4))
	x._Reserved1 = (uint64)(C.get_bitfield_perf_event_attr___reserved_1(x.refe590ab4))
	x.BpType = (uint32)(x.refe590ab4.bp_type)
	x.BranchSampleType = (uint64)(x.refe590ab4.branch_sample_type)
	x.SampleRegsUser = (uint64)(x.refe590ab4.sample_regs_user)
	x.SampleStackUser = (uint32)(x.refe590ab4.sample_stack_user)
	x.ClockID = (int32)(x.refe590ab4.clockid)
	x.SampleRegsIntr = (uint64)(x.refe590ab4.sample_regs_intr)
	x.AuxWatermark = (uint32)(x.refe590ab4.aux_watermark)
	x.SampleMaxStack = (uint16)(x.refe590ab4.sample_max_stack)
	x._Reserved2 = (uint16)(x.refe590ab4.__reserved_2)
}
